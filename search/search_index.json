{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#ct-fm-a-3d-image-based-foundation-model-for-radiological-tasks","title":"CT-FM: A 3D Image-Based Foundation Model for Radiological Tasks","text":""},{"location":"#brief","title":"Brief","text":"<p>This repository contains the code and resources for CT-FM, a 3D image-based pre-trained foundation model designed for various radiological tasks. CT-FM is trained using self-supervised learning (SSL) on a large dataset of 148,000 CT scans. This model aims to address a range of tasks, including whole-body segmentation, tumor segmentation, head CT triage, and medical image retrieval. This work builds upon previous efforts in radiological AI, shifting from task-specific expert models to unified foundation models for broader adaptability and efficiency.</p>"},{"location":"#key-innovations","title":"Key Innovations","text":"<ul> <li>Large-Scale 3D Pretraining: Emphasis on 3D data rather than traditional 2D datasets.</li> <li>Task-Agnostic Training: Enabling transferability across various radiological tasks.</li> <li>Open Source: Model weights, data, and code are shared for collaborative development.</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p>Downloading Data</p> <p>All datasets used in the study are public</p> <p> Download data</p> </li> <li> <p>Use CT-FM models</p> <p>CT-FM feature extractors and trained downstream  models are available on HF</p> <p> Go to HF</p> </li> <li> <p>Reproduce our pre-training framework</p> <p>Implement our pre-training method on your own data</p> <p> Pretraining instructions</p> </li> <li> <p>Build your projects using Lighter</p> <p>CT-FM \u2665 Lighter  All pre-training is performed using lighter  Explore here</p> </li> </ul>"},{"location":"getting-started/using_ctfm/","title":"Using CT-FM in your projects","text":""},{"location":"getting-started/using_ctfm/#install-lighter-zoo","title":"Install Lighter-Zoo","text":"<p>Our CT-FM models are available via a pip package API hosted on Hugging Face. This streamlined API enables you to effortlessly extract features and generate predictions for various radiological tasks.</p> <p>Begin by installing the lighter-zoo package:</p> <pre><code>pip install lighter-zoo\n</code></pre> <p>Quick Start</p> <p>For detailed examples and further guidance, visit our Project Lighter page on Hugging Face</p>"},{"location":"getting-started/using_ctfm/#available-models","title":"Available Models","text":"<ul> <li> <p> project-lighter/ct_fm_feature_extractor   Extract deep features efficiently from CT scans.</p> </li> <li> <p> project-lighter/ct_fm_segresnet   Load a CT-FM SegResNet that you can use for finetuning in your projects</p> </li> <li> <p> project-lighter/whole_body_segmentation   Generate comprehensive segmentation maps for radiological analysis.</p> </li> </ul>"},{"location":"replication-guide/analysis/","title":"Reproduce Analysis","text":"<ul> <li> <p>Whole Body Segmentation Analysis:  totalseg_eval.ipynb</p> </li> <li> <p>Tumor Segmentation Analysis and Visualization:   tumor-seg-eval</p> </li> <li> <p>Head CT Triage Classification:   head-ct-triage-eval</p> </li> <li> <p>Medical Image Retrieval:  retrieval</p> </li> <li> <p>Semantic Evaluation - Anatomical Clustering, Semantic Search, PCA visualization:  semantic-eval</p> </li> <li> <p>Robustness - Saliency and Stability:  robustness</p> </li> </ul>"},{"location":"replication-guide/baselines/","title":"Baselines","text":"<p>The selection of baselines varies depending on the evaluation task. Below is a breakdown by task:</p>"},{"location":"replication-guide/baselines/#whole-body-segmentation","title":"Whole Body Segmentation","text":"<ul> <li>Architectural Baseline: Randomly initialized model.</li> <li>SuPREM</li> <li>Merlin</li> <li>VISTA3D, Auto3DSeg, nnUNet: Results reported in previous studies.</li> </ul>"},{"location":"replication-guide/baselines/#tumor-segmentation","title":"Tumor Segmentation","text":"<ul> <li>Auto3DSeg Pipeline</li> </ul>"},{"location":"replication-guide/baselines/#head-ct-triage","title":"Head CT Triage","text":"<ul> <li>Architectural Baseline: Randomly initialized model.</li> <li>SuPREM</li> </ul> <p>All baselines\u2014as well as our methods\u2014are implemented using lighter. For detailed configuration scripts and execution instructions, please refer to the Downstream Tasks section.</p>"},{"location":"replication-guide/data/","title":"CT-FM Data Download Guide","text":"<p>This guide details the steps to download and process the CT scan data used for both pre-training and downstream tasks in the CT-FM study. All datasets are publicly available, ensuring that every element of our study can be reproduced.</p>"},{"location":"replication-guide/data/#pre-training-data","title":"Pre-training Data","text":"<p>For our pre-training experiments, we utilize 148,394 CT scans from the Imaging Data Commons (IDC). Follow the steps below to obtain and prepare the exact dataset used in our study.</p>"},{"location":"replication-guide/data/#1-run-the-data-query-in-bigquery","title":"1. Run the Data Query in BigQuery","text":"<p>Execute the provided SQL query on Google BigQuery to filter for CT scans that meet our quality constraints. The query performs necessary quality checks on each scan.</p> <p> Query file</p> <p>Running this query returns a table with CT scan records that satisfy our criteria. We then convert these query results to a manifest file that can be used to download the data</p>"},{"location":"replication-guide/data/#2-generate-a-download-manifest","title":"2. Generate a Download Manifest","text":"<p>This has already been done so you can skip to the next step if you don't want to know how!</p> <p>After reviewing the query results, use the Jupyter Notebook to create a manifest file. This manifest lists every DICOM file that needs to be downloaded.</p> <p> Manifest creation notebook</p>"},{"location":"replication-guide/data/#3-download-the-dicom-files","title":"3. Download the DICOM Files","text":"<p>To download the scans, first install the IDC Index tool:</p> <pre><code>pip install idc-index\n</code></pre> <p>Then, execute the following command\u2014replacing <code>&lt;PATH_TO_MANIFEST.TXT&gt;</code> and <code>&lt;DOWNLOAD_DIR&gt;</code> with your manifest file path and desired download directory:</p> <pre><code>idc download-from-manifest --manifest-file &lt;PATH_TO_MANIFEST.TXT&gt; --download-dir &lt;DOWNLOAD_DIR&gt;\n</code></pre> <p>This command downloads all the specified DICOM files into the designated directory.</p>"},{"location":"replication-guide/data/#4-sort-and-convert-the-data","title":"4. Sort and Convert the Data","text":"<p>The downloaded data is in DICOM format. To prepare it for your experiments, follow these steps:</p> <ul> <li> <p>Sorting: Organize the DICOM files using the tool \"dicomsort\". While the specific usage may depend on your environment, a common workflow involves running a command to categorize files by patient or study. For example, you might first list the files and then run:</p> <pre><code>dicomsort [options...] sourceDir targetDir/&lt;patterns&gt;\n</code></pre> <p>For more detailed instructions and options, please refer to the thedicomsort GitHub repository </p> </li> <li> <p>Conversion: Convert the sorted DICOM files to NRRD format using Plastimatch. A typical command looks similar to:</p> <pre><code>plastimatch convert --input &lt;SORTED_DIR&gt; --output &lt;CONVERTED_DIR&gt; --format nrrd\n</code></pre> <p>For additional details and advanced options, consult the Plastimatch documentation or relevant online resources.</p> </li> <li> <p>Packaging: Finally, generate a <code>.pkl</code> file that lists the scans. This file serves as the required input for the pre-training experiments.</p> </li> </ul> <p>For a complete example of these final steps, refer again to the prepare_download.ipynb notebook.</p> <p>Following these instructions will replicate the data download and preprocessing pipeline used in our study, enabling you to work with the same CT scan dataset.</p>"},{"location":"replication-guide/data/#downstream-tasks-data","title":"Downstream Tasks Data","text":"<p>We use several publicly available datasets for our downstream tasks, including:</p> <ul> <li> <p>Whole Body Segmentation:  TotalSegmentator-v2 dataset</p> </li> <li> <p>Tumor Segmentation:  MSD dataset</p> </li> <li> <p>Head CT Triage:  SinoCT  CQ500</p> </li> <li> <p>Medical Image Retrieval:  3D-MIR  OrganMNIST-3D</p> </li> <li> <p>Stability Testing:  RIDER</p> </li> </ul>"},{"location":"replication-guide/downstream/","title":"Downstream Task Adaptation","text":"<p>Our pre-trained CT-FM model has been adapted to three fine-tuned downstream tasks as well as several additional zero\u2010shot tasks. While most downstream experiments leverage the Lighter framework, tumor segmentation is handled using Auto3DSeg.</p>"},{"location":"replication-guide/downstream/#whole-body-segmentation","title":"Whole Body Segmentation","text":"<p>In line with the configuration-based approach detailed in Pretraining, we provide YAML config files for downstream adaptation. To facilitate thorough comparisons, a suite of shell scripts with the relevant configuration components is available. These can be found in the evaluation directory under \u201cscripts.\u201d</p> <p> View All Scripts</p> <p> For TotalSeg experiments, refer to the scripts in the totalseg folder:</p> <ul> <li> <p>Full Finetuning on TotalSegmentatorV2:  fulltune.sh</p> </li> <li> <p>Finetuning on the Merlin Split:  merlin.sh</p> </li> <li> <p>Few-Shot Fine-Tuning:  fewshot.sh</p> </li> <li> <p>Pre-Training Checkpoint Selection:  checkpoint_selection.sh</p> </li> <li> <p>Pre-Training Ablations:  pretraining_evaluation.sh</p> </li> </ul> <p>Enabling Prediction Mode</p> <p>To switch from training to prediction mode: - Replace the <code>fit</code> command with the <code>predict</code> command. - Append the prediction override configuration file <code>./evaluation/overrides/totalseg_predict_overrides.yaml</code> to your config list. - Remove the <code>--trainer#callbacks#0#until_epoch=0</code> flag since the new callback now handles prediction mode.</p> <p>Example Transformation:</p> <p>Original command: <pre><code>lighter fit --config=./evaluation/totalseg.yaml,./evaluation/overrides/totalseg_vista.yaml,./evaluation/baselines/segresnetds_ctfm.yaml --trainer#callbacks#0#until_epoch=0 --vars#name=\"ct_fm\" --vars#project=\"totalseg\" --system#model#trunk#ckpt_path=$ct_fm_path --vars#wandb_group='vista_v2'\n</code></pre></p> <p>Modified prediction command: <pre><code>lighter predict --config=./evaluation/totalseg.yaml,./evaluation/overrides/totalseg_vista.yaml,./evaluation/baselines/segresnetds_ctfm.yaml,./evaluation/overrides/totalseg_predict_overrides.yaml --vars#name=\"ct_fm\" --vars#project=\"totalseg\" --vars#wandb_group='vista_v2'\n</code></pre></p> <p>By default the predict command uses the checkpoint location mentioned while running the fit pipeline. If you have a different checkpoint location, to override the model checkpoint directory during prediction, add: <pre><code>--args#predict#ckpt_path=&lt;path&gt;\n</code></pre></p>"},{"location":"replication-guide/downstream/#tumor-segmentation-with-auto3dseg","title":"Tumor Segmentation with Auto3DSeg","text":"<p>Tumor segmentation is performed using Auto3DSeg\u2014a robust segmentation workflow provided by MONAI. This pipeline is designed to simplify segmentation tasks and can be explored further in the official link below</p> <p> MONAI Auto3DSeg Tutorial</p>"},{"location":"replication-guide/downstream/#workflow-overview","title":"Workflow Overview","text":"<p>Auto3DSeg operates by running an AutoRunner that takes a configuration file (typically named task.yaml) as input. This file contains all the necessary parameters to handle preprocessing, training, and validation stages of your segmentation task.</p>"},{"location":"replication-guide/downstream/#model-details","title":"Model Details","text":"<p>Our experiments focus on the segresnet_0 model variant, which is set up for single-fold training and validation. We run the baseline model using the default Auto3DSeg configuration. However, when integrating our CT-FM model into the pipeline, we make the following two key modifications:</p> <ul> <li> <p>Orientation Adjustment:   We change the default image orientation by setting the axcodes to <code>SPL</code>.</p> </li> <li> <p>Checkpoint Specification:   The path to the pre-trained model checkpoint is provided via the <code>ckpt_path</code> field in the hyper_parameters.yaml file.</p> </li> </ul> <p>These adjustments allow us to directly benchmark the effectiveness of the pre-trained CT-FM model within the Auto3DSeg pipeline without necessitating major changes to the existing workflow.</p> <p>Customizing Your Pipeline</p> <p>By simply modifying the orientation and specifying the checkpoint path, you can leverage the power of pre-trained models in the Auto3DSeg setup. This makes it easy to compare different configurations and accelerate your experimentation process.</p>"},{"location":"replication-guide/downstream/#head-triage-ct-classification","title":"Head Triage CT classification","text":"<p> Coming soon...</p> <p> </p> <p>Zero-shot evaluation</p> <p>All the zero shot eval can be found on the reproduce analysis page</p>"},{"location":"replication-guide/installation/","title":"Installation","text":"<p>Follow these steps to install CT-FM on your local machine:</p> <ol> <li> <p>Clone the Repository    Open a terminal and run:    <pre><code>git clone https://github.com/project-lighter/CT-FM.git\n</code></pre></p> </li> <li> <p>Navigate to the Project Directory    Change into the cloned repository\u2019s directory:    <pre><code>cd CT-FM\n</code></pre></p> </li> <li> <p>Install the Dependencies    Install all required packages (including lighter) with:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol> <p>After completing these steps, CT-FM and all its dependencies will be installed and ready for use.</p>"},{"location":"replication-guide/pretraining/","title":"Pre-training CT-FM","text":"<p>Before you begin, ensure you have downloaded your data as explained in the Data Instructions. It is also a good idea to review the lighter documentation since our training configurations are based on its guidelines.</p>"},{"location":"replication-guide/pretraining/#pre-training-experiment-configurations","title":"Pre-training Experiment Configurations","text":"<p>Pre-training configuration files are organized in the experiments directory. Explore the key folders below to find the setup that best meets your needs:</p> <ul> <li> <p> Ablations Setups for testing various experimental approaches.</p> </li> <li> <p> FM Finalized configurations for the pre-training run.</p> </li> </ul>"},{"location":"replication-guide/pretraining/#running-the-pretraining","title":"Running the pretraining","text":"<p>After all adjustments have been made, navigate to the root directory of the CT-FM project and execute the following command to begin pre-training:</p> <pre><code>lighter fit --config=./experiments/fm/base.yaml,\\ #(1)!\n./experiments/fm/frameworks/intrasample_simclr.yaml,\\ #(2)!\n./experiments/fm/backbones/segresenc.yaml #(3)!\n</code></pre> <ol> <li> <p>The  file establishes the core settings for pre-training the CT-FM model by defining:</p> <ul> <li>Variables: Core parameters such as voxel spacing.</li> <li>Trainer Settings: Parameters including 500 epochs, batch limits, GPU configuration, mixed precision, logging via WandB, and checkpoint callbacks.</li> <li>System Settings: The model placeholder, optimizer (AdamW), learning rate scheduler (WarmupCosineSchedule), and dataloader setup for safely handling your dataset.</li> <li>Adapters: Methods for batch processing and loss computation.   In essence, <code>base.yaml</code> serves as the foundation upon which the entire pre-training process is built.</li> </ul> </li> <li> <p>The  file configures the self-supervised SimCLR framework used during pre-training. It includes:</p> <ul> <li>Model &amp; Criterion: Defines the CT-FM model and applies a contrastive loss function with a specified temperature.</li> <li>Data Augmentation Pipeline: Implements a series of transformations (such as random crops, flips, and intensity adjustments) to generate multiple augmented views from each input image.   This configuration augments the base setup with specialized self-supervised learning components.</li> </ul> </li> <li> <p>The  file sets up the backbone for the CT-FM model. It includes:</p> <ul> <li>Backbone Identification: Sets the variable <code>BACKBONE_NAME</code> to <code>\"SegResNetDS\"</code>.</li> <li>Architectural Details: Configures the SegResNet encoder (via <code>monai.networks.nets.segresnet_ds.SegResEncoder</code>) by specifying parameters like spatial dimensions, input channels, initial filters, and block structures.</li> <li>Integration with Base Config: Uses shared variable mappings (such as <code>NUM_FTRS_BY_BACKBONE</code>) and logger identifiers from <code>base.yaml</code> to ensure smooth integration.   This configuration provides the essential backbone architecture for complete model training.</li> </ul> </li> </ol> <p>Click on the  symbols to learn more about each yaml file</p>"},{"location":"replication-guide/pretraining/#customization-before-training","title":"Customization Before Training","text":"<p>Before running the experiment, update your <code>base.yaml</code> configuration using the guidelines below:</p> <p>Directory Paths Update</p> <ul> <li>Set the paths for <code>save_dir</code> and <code>dirpath</code> to your preferred locations for saving logs and checkpoints.</li> <li>Update the path for <code>scan_list.pkl</code> to reflect the file produced during the data preparation phase.</li> </ul> <p>Training Parameter Adjustments</p> <p>Modify the settings under the <code>trainer:</code> key (such as the number of GPUs, batch size, and training duration) to align with your system\u2019s resources and experimental needs.</p> <p>After applying these customizations, execute the pre-training command to initiate the process with your updated configurations.</p>"}]}