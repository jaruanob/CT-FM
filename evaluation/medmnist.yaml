project: .

_requires_:
    - "$import monai"
    - "$import torch"
    - "$import medmnist"
    - "$import numpy as np"

vars:
    init_LR: 0.001
    in_channels: 1
    embedding_dim: 512
    wandb_group: "%#dataset"
    format: "$'monai' if 'suprem' in @vars#name or 'vista' in @vars#name else 'lighter'"
    # Transforms the MedMNIST format to either lighter format (SPL, default) or SuPREM format (RAS)
    dataset_specific_transform: "$lambda dataset: [{'input': item[0].astype(np.float32).transpose(0, 3, 2, 1) if @vars#format == 'lighter' else np.flip(item[0].astype(np.float32), axis=[2, 1]).copy(), 'target': item[1][0]} for item in dataset]"
    save_dir: '$f"/media/volume/CT-RATE/CT-FM/evaluations/{@vars#project}/checkpoints/{@vars#name}_{@vars#wandb_group}"'

args:
  validate:
    ckpt_path: $f"{@trainer#callbacks#1#dirpath}/best.ckpt"
  test: "%#validate"

trainer:
    _target_: pytorch_lightning.Trainer
    benchmark: True
    max_epochs: 100
    check_val_every_n_epoch: 1
    accelerator: gpu
    # ---------
    devices: 1
    # strategy: ddp_find_unused_parameters_true
    # sync_batchnorm: True
    # ---------
    precision: 16-mixed
    log_every_n_steps: 10
    logger:
        _target_: pytorch_lightning.loggers.WandbLogger
        project: "@vars#project"
        name: '$f"{@vars#name}"'
        save_dir: '$f"/media/volume/CT-RATE/CT-FM/{@vars#project}/logs/{@vars#dataset}_{@vars#name}"'

    callbacks:
        - _target_: lighter.callbacks.LighterFreezer
          # _orig_mod is torch.compile added notation
          name_starts_with: ["trunk"]
        - _target_: pytorch_lightning.callbacks.ModelCheckpoint
          dirpath: "@vars#save_dir"
          save_last: False
          monitor: "val/metrics/MulticlassAUROC/epoch"
          mode: "max"
          filename: "best"
          auto_insert_metric_name: False
          verbose: True
          every_n_epochs: 1
system:
    _target_: lighter.LighterSystem
    batch_size: 32
    pin_memory: True
    num_workers: 8
    model:
        _target_: project.models.wrapper.TrunkHeadWrapper
        trunk:
            _target_: lighter.utils.model.adjust_prefix_and_load_state_dict
            ckpt_path: 
            ckpt_to_model_prefix: '${"backbone.encoder" : ""} if "recon" in @vars#name else {"backbone.": ""}'
            # ckpt_to_model_prefix: '${"_orig_mod.backbone.encoder" : ""} if "recon" in @vars#name else {"_orig_mod.backbone.": ""}'
            model:
                _target_: monai.networks.nets.segresnet_ds.SegResEncoder
                spatial_dims: 3
                in_channels: "@vars#in_channels"
                init_filters: 32
                blocks_down: [1, 2, 2, 4, 4]

        head:
            - _target_: torch.nn.AdaptiveAvgPool3d
              output_size: 1
            - _target_: torch.nn.Flatten

            ## Mimic FC structure from paper: 
            # https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb
            - _target_: torch.nn.Linear
              in_features: "@vars#embedding_dim"
              out_features: 128
            - _target_: torch.nn.ReLU
            - _target_: torch.nn.Linear
              in_features: 128
              out_features: 128
            - _target_: torch.nn.ReLU
            - _target_: torch.nn.Linear
              in_features: 128
              out_features: "@vars#num_classes"
        pre_func: 
            - "$lambda x: x[-1]"
            
    criterion:
        _target_: "$torch.nn.CrossEntropyLoss if @vars#num_classes > 1 else torch.nn.BCEWithLogitsLoss"

    optimizer:
        _target_: torch.optim.AdamW
        params: "$@system#model.parameters()"
        lr: "@vars#init_LR"
        weight_decay: 1.0e-05 

    scheduler:
        _target_: torch.optim.lr_scheduler.MultiStepLR
        optimizer: "@system#optimizer"
        milestones: [50, 75]

    metrics:
        train:
            - _target_: torchmetrics.AUROC
              task: multiclass # Note: Change to `binary` for Task 2 and Task 3 and remove num_classes below
              num_classes: "@vars#num_classes"
        val: "%#train"
        test: "%#train"

    datasets:
        train:
            _target_: monai.data.DatasetFunc
            data:
                _target_: "@vars#dataset"
                split: "train"
                download: True
                size: 64

            # Func converts the dataset to lighter format and transposes input so it follows SPL convention                
            func: "@vars#dataset_specific_transform"

        val:
            _target_: monai.data.DatasetFunc
            data:
                _target_: "@vars#dataset"
                split: "val"
                download: True
                size: 64

            # Func converts the dataset to lighter format and transposes input so it follows SPL convention
            func: "@vars#dataset_specific_transform"

        test:
            _target_: monai.data.DatasetFunc
            data:
                _target_: "@vars#dataset"
                split: "test"
                download: True
                size: 64

            # Func converts the dataset to lighter format and transposes input so it follows SPL convention
            func: "@vars#dataset_specific_transform"
            
    postprocessing:
        metrics:
            pred: # Pred for torchmetrics is (N, C, ...)
                - "$lambda x: torch.softmax(x, 1)"

            target:
                # Remove the channel dim. Target for torchmetrics is (N, ...)
                - "$lambda tensor: tensor.long()"
        logging:              
            pred:
                - "$lambda x: x.argmax(dim=1, keepdim=True)"
                - "$lambda x: x.float()"
            target:
                - "$lambda x: x.unsqueeze(1)"
                - "$lambda x: x.float()"
