CONSTANTS:
    NUM_FTRS_BY_BACKBONE: {"SegResNetDS": 512, "ResNet50x2": 4096}

project: .

trainer:
    _target_: pytorch_lightning.Trainer
    benchmark: True
    max_epochs: 500
    limit_train_batches: 500
    accelerator: gpu
    devices: 4
    strategy: ddp
    sync_batchnorm: True
    precision: 16-mixed
    log_every_n_steps: 10
    logger: 
        _target_: pytorch_lightning.loggers.WandbLogger
        name: $f"CT_FM_{@CONSTANTS#pretrain_method}_{@CONSTANTS#backbone_name}"
        project: CT_FM
        save_dir: $f"/mnt/data16/ibro/IDC_SSL_CT/runs/logs/CT_FM_{@CONSTANTS#pretrain_method}_{@CONSTANTS#backbone_name}"
  
    callbacks:
        - _target_: pytorch_lightning.callbacks.ModelCheckpoint
          dirpath: $f"/mnt/data16/ibro/IDC_SSL_CT/runs/checkpoints/CT_FM_{@CONSTANTS#pretrain_method}_{@CONSTANTS#backbone_name}"
          save_last: True
          verbose: True
          every_n_epochs: 1 # 10
          save_weights_only: False  # True

system:
    _target_: lighter.LighterSystem
    batch_size: 4
    pin_memory: True
    drop_last_batch: True # Used in SSL cases because of negatives
    num_workers: 6 # 32
  
    optimizer:
        _target_: torch.optim.AdamW
        params: "$@system#model.parameters()"
        # Learning rate calculated as per: `lr = (effective_batch_size) / 256 * base_lr`
        lr: "$((@system#batch_size * @trainer#devices)/256) * 0.0007"
        weight_decay: 0.000001
  
    scheduler:
        _target_: monai.optimizers.WarmupCosineSchedule
        optimizer: "@system#optimizer"
        warmup_steps: 10  # First 10 epochs
        t_total: $@trainer#max_epochs
  
    datasets:
        train:
            _target_: monai.data.Dataset
            data: "$pickle.load(open('/mnt/data16/ibro/IDC_SSL_CT/dataset_list.pkl', 'rb'))"
            _requires_: "$import pickle"
            transform:

